\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{times}
\usepackage[scientific-notation=true]{siunitx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
%\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[normalem]{ulem}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\phi}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}
%\newcommand{\log}{\text{log}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}

\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mP}[0]{\matr{P}}

\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\PP}[0]{\vects{\phi}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\NN}[0]{\mathcal{N}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\text{sigmoid}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}


\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

%\newcommand\codeHighlight[1]{\textcolor[rgb]{1,0,0}{\textbf{#1}}}
\newcommand\codeHighlight[1]{\textcolor[rgb]{1,0,0}{#1}}







%\title{Larger-Context Language Modelling \\ with Recurrent Neural Network}
\title{Iterative Refinement of Approximate Posterior for Training Directed Belief Networks}

\author{R. Devon Hjelm \\
Mind Research Network \\
and the Department of Computer Science \\
University of New Mexico \\
\texttt{dhjelm@mrn.org} \\
\And
Nebojsa Jojic \\
Mictrosoft Resarch \\
\texttt{jojic@microsoft.com}
\And
Kyunghyun Cho \\
Courant Institute of Mathematical Sciences \\ 
and Center for Data Science \\
New York University \\
\texttt{kyunghyun.cho@nyu.edu}
\And
Junyoung Chung \\
University of Montreal \\
\texttt{elecegg@gmail.com}
\And
Ruslan Salakhutdinov \\
University of Toronto \\
\texttt{rsalakhu@cs.toronto.edu}
\And
Vince Calhoun \\
Mind Research Network \\
and the Department of Electrical \\ 
and Computer Engineering \\
University of New Mexico \\
\texttt{vcalhoun@mrn.org}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
    Deep directed graphical models, while representing a potentially powerful
    class of generative representations, are challenging to train due to
    difficult inference. Recent advances in variational inference that make use
    of a inference or \emph{recognition network} have advanced well beyond
    traditional variational inference and MCMC methods. While these techniques
    offer higher flexibility as well as simpler and faster inference, they are
    still limited by the choice of approximate posterior and require variance
    reduction. Less focus has been given to improving or \emph{refining} the
    approximate posterior beyond those provided by variational inference. We
    show that iterative refinement of the approximate posterior can provide
    notable gains in maximizing the lower bound of the log likelihood, either
    by applying gradient descent or by using adaptive importance sampling
    during the E-step of a variational EM algorithm. We show our approach
    competes with state of the art in both continuous and binary latent
    variables.
    \end{abstract}

\section{Introduction}

Deep generative models offer the capacity for rich representations of complex
data while as they can capture multimodal properties of the distribution of
interest and generalize better. Deep directed
graphical models in particular have some advantage over their undirected
cousins such as DBMs \citep{salakhutdinov2009deep} as they do not suffer from
learning and evaluation difficulties arising from an intractable partition
function. Despite their representational power, directed graphical models are
generally difficult to train, as the true posterior is generally intractable.

Methods for training variants of the Helmholtz machine
\citep{dayan1995helmholtz}, such as wake-sleep \citep{hinton1995wake,
bornschein2014reweighted} and neural variational inference and learning (NVIL)
\citep{mnih2014neural}, and methods for training variational autoencoders (VAE)
\citep{kingma2013auto} have supplanted more traditional Markov Chain Monte
Carlo (MCMC) or variational inference based methods such as mean field
coordinate ascent inference in training deep generative models. The gains
offered by these methods are due to parameterizing the approximate posterior
with a \emph{recognition network} on top of clever learning algorithms. As
opposed to traditional methods, this general approach provides a fast and
effective means of performing inference and learning which scales well to large
datasets.

While advances have been clear, these methods are still limited by the choice
of approximate posterior, which translates to a looseness in the lower bound.
Some recent work has begun to address this limitation by either relaxing the
factorial assumption \citep{burda2015importance} on the approximate posterior or
through the introduction of auxiliary inference networks
\citep{rezende2015variational}. 

\alert{I need to work on this} Notably, there has been recent work focusing on
refining the variational posterior by applying gradient descent to variational
inference \citep{hoffman2013stochastic} or by applying additional MCMC
transitions \citep{salimans2014markov}. We apply two different techniques to
refine the approximate posterior from the recognition network during the E-step
of an EM algorithm. We first show that, given lower bound optimization can be
reinterpreted in terms of a deterministic function, we can use gradient descent
to refine the approximate posterior during inference. Next, we show that with
binary latent variables, adaptive importance sampling (AdIS)
\citep{karamchandani1989adaptive} can be used iteratively to achieve a tighter
lower bound to the log likelihood.

\section{Directed Belief Networks and Variational Inference}

The term \emph{belief network} describes a class of generative models of which
the observed distribution is conditioned on a set of stochastic latent causes:
$p(x) = \sum_h p(x|h) p(h)$, though in general this term is used to include
models where the observed variables are not conditionally independent on the
latent variables, $h$. 

In this work, we will use the name \emph{directed belief network} to restrict
ourselves to a generative directed graphical model such that the likelihood
function factorizes into a hierarchy of conditional densities: $p(x | h) =
p(h_n) p(x|h_1) \prod_{l=1}^n p(h_{l}|h_{l+1})$, where each layer $l$ is
conditionally independent on the layer above with density $p(h_{l}|h_{l + 1})$
and $p(h_n)$ is a prior distribution of the top layer. Examples include sigmoid
belief networks \citep[SBN,][]{neal1992connectionist} and deep belief networks
\citep[DBN,][]{hinton2006fast}.

Directed belief networks can be trained by maximizing log-likelihood (MLE), but
this is difficult due to the necessary marginalization over the latent density.
If we had the posterior $p(h|x) = p(x, h) / p(x)$, then learning would be easy,
but in general this is not possible and we require some more advanced
techniques. 

\section{Variational Inference and Learning of Directed Belief Networks}
\subsection{Variational Lowerbound of Directed Belief Network}

Even with reasonably complex likelihood functions, MLE is not tractable in
directed belief networks due to the intractable posterior inference.  One
solution is to use Markov Chain Monte Carlo (MCMC) sampling to iteratively
approximate the posterior distribution over the latent variables, which is exact
given an unbound number of steps under certain conditions. This typically
suffers high variance. 
%from poor mixing in the transition operator and high variance. 

Another popular method, variational inference introduces an approximate
posterior $q_{\TT}(h|x)$:
\begin{align}
    \label{eq:approx_logp}
    \log p(x) =& \log \sum_{h} p(x, h) 
    = \log \sum_h q(h|x) \frac{p(x, h)}{q(h|x)} \nonumber \\
    \geq& \sum_h q(h|x) \log \frac{p(x, h)}{q(h|x)} 
    = \sum_h q(h|x) \log p(x,h) + \HH(q) = \LL,
\end{align}
with variational parameters $\TT$, where $\HH(q)$ is the entropy of the
approximate posterior $q$. 

This introduces \emph{lower bound}, $\LL$, of the exact likelihood. Variational
inference rephrases learning as choosing the variational parameters $\TT$ to
maximize this bound, rather than choosing the latent variables directly. The
bound is tight (i.e., $\LL = \log p(x)$) when the KL divergence between the
approximate and true posterior is $0$ (i.e., $D_{KL}(q_{\TT}(h|x)||p(h|x)) =
0$,) or equivalently when the approximate matches the true posterior.
Optimization then is dependent on the choice of a parametric form for the
posterior that can best approximate the exact posterior, and models generally
vary in choice of parameterization followed by a learning algorithm which
efficiently maximizes its lower bound.

\subsection{Training a Directed Belief Network}

If we have a method for finding an approximate posterior, then an
expectation-maximization (EM)
algorithm~\citep{dempster1977maximum,neal1998view}.  In the Expectation (E)
step, the variational parameters parameters of the approximate posterior, $\TT$,
are chosen/updated such that the KL-divergence between the approximate posterior and
true posterior is minimized. In the Maximization (M) step, the model parameters
$\PP$ are chosen/updated to maximize the likelihood function.

In general, the gradient w.r.t. the model parameters of the lower bound can be
estimated using the following Monte Carlo approximation:
\begin{align}
\nabla_{\PP} \LL \approx \frac{1}{N} \sum_n \nabla_{\PP} \log p_{\PP}(x, h^{(n)}),
\end{align}
where $h^{(n)} \sim q(h|x)$.

\subsection{Recognition Network}

Recently, it has been found that the approximate posterior inference can be done
quickly by using a feed forward, often deep, \emph{recognition network} composed
of \emph{global} variational parameters to predict the local variational
parameters from the data \citep[see,
e.g.,][]{kingma2013auto,mnih2014neural,rezende2014stochastic}. However, these
approaches often require additional tricks to train, as the gradient of the
lower bound w.r.t. the global variational parameters is difficult to compute,
and most of the off-the-shelf approximations, such as Monte Carlo approximation,
have high variance. 

Variational autoencoders (VAE) \citep{kingma2013auto} make use of a clever
parameterization trick so that the lower bound can be rephrased as a
deterministic function with some auxilliary noise. The wake-sleep algorithm uses
samples from the joint density $p(x, h)$ to optimize the variational parameters,
effectively making the algorithm an optimization over two cost functions
\citep{hinton1995wake}. NVIL
uses REINFORCE along with a baseline predicted from a deep neural network to
make lower variance MCMC estimates of the gradient.

It can be argued that better estimates can be made by simply having a better
posterior. Some work has proposed to use gradient descent
\citep{hoffman2013stochastic}, auxiliary networks
\citep{rezende2015variational}, or MCMC transitions \citep{salimans2014markov}
to refine inference with various posterior parameterizations. 

In general, we want an iterative refinement procedure, defining a transition
operator $T$ that arrives at a better posterior, $\tilde{q}$, when applied
iteratively to approximate posterior, $q$. 
%We then adjust our global variational
%parameters by minimizing the KL divergence, $D_{KL}(\tilde{q}||p)$.

\section{Iterative Refinement for Variational Inference}

\subsection{Iterative Refinement of Approximate Posterior}
We restrict ourselves to the case where the prior and approximate posterior are
factorial, that is: $p(h) = \prod_i p(h_i)$ and $\tilde{q}(h|x; \TT) = \prod_i
\tilde{q}(h_i|x; \TT)$, respectively. In
order to augment our inference from those available from the parameterized
approximate posterior, we wish to achieve a better posterior by taking a set
number of iterative steps in the \emph{local} variational parameters $\TT^l$
from variation inference by applying a series of transition operators
$T(\TT^l_t, \TT^l_{t-1}; \LL)$ parameterized by $\LL$. These transition
operators should be a function of the lower bound and maximize it as well.

After we find $\tilde{q}$, we maximize the lower bound w.r.t. the model parameters using stochastic gradient descent, using samples from the refined approximate posterior. Our loss w.r.t. the global variational parameters becomes $D_{KL}(\tilde{q}(\theta^l)||q_{\TT})$.

\subsubsection{Gradient Descent Iterative Refinement (GDIR)}
If we can rephrase the lower bound as a deterministic function $g$ of the local
variational parameters and some optional auxiliary noise variables, $\epsilon$,
then we can improve the local variational parameters by applying iterative steps
of gradient ascent:

\begin{align}
\grad_{\TT^l} \LL &\approx \grad_{\TT^l} g(\TT^l, \epsilon) \nonumber \\
\TT^l_t &= \TT^l_{t-1} + \gamma \grad_{\TT^l} g(\TT^l, \epsilon)
\end{align}
where $\gamma$ is the "inference rate" and $\TT^l_0$ are the initial local
parameters found through the initial variational inference. The success of this
approach will depend on the choice of approximation, which we will show.

\paragraph{Gaussian Latent Variables}

Let us assume that our approximate posterior and prior distributions are
parameterized by multivariate normal distributions with diagonal covariance. In
order to pass the gradient of the approximate log conditional to the variational
parameters, we can use the re-parameterize our approximate posterior as in VAE:
\begin{align}
h(\TT^l) &= \mu + \epsilon \odot \sigma \nonumber \\
\epsilon &\sim \NN(0, 1),
\end{align}
where $(\mu, \sigma) = \TT^l$ are the mean and variance parameters for the
posterior density and our auxiliary noise variables, $\epsilon$, are sampled
from a unit-variance, zero-mean normal distribution.

The gradient of the lower bound w.r.t. the local variational parameters can be
approximated easily by Monte Carlo method:
\begin{align}
\grad_{\TT^l} g(\TT^l, \epsilon) = \frac{1}{N} \grad_{\TT^l} \bigg(\sum_n \log p(x, h(\TT^l)^{(n)}) - \log \tilde{q}(h(\TT^l)^{(n)})\bigg)
\end{align}

\paragraph{Bernoulli Latent Variables}
Unfortunately, a suitable re-parameterization as above is not available to us
with binary latent variables. In order to use gradient descent to back-propagate
the gradients of the lower bound, we need to make a stronger approximation. 

There has been some recent work into passing gradients through binary latent
variables \citep{bengio2013estimating}. Instead of approximating the partial
derivatives w.r.t. the latent variables in back-propagation, we instead push the
centers of the Bernoulli distribution, avoiding sampling overall. Our lower
bound becomes:
\begin{align}
     \LL \approx g(\TT^l) = \log p(x | \mu) + \qexp{\log p(h)}_{\tilde{q}} +
     \HH(\tilde{q}),
\end{align}
where $\log p(x | \mu)$ is the log-output of the generation network using the
variational parameters $\mu = \sigmoid(z)$ as input. However, this approximation
is likely highly biased, and becomes worse as $\mu$ moves away from $(0, 1)$, so
we would expect this approximation to be best when the entropy is low.

\subsubsection{Adaptive Importance Sampling Iterative Refinement (AdSIR)}
As an alternative and to address the issues with binary latent variables, we use \emph{adaptive importance sampling} \citep{karamchandani1989adaptive} to iteratively refine the Bernoulli centers at each inference step:

\begin{align}
    h^{(n)} &\sim \Bernoulli(\mu_t) \nonumber \\
w^{(n)} &= p(x, h^{(n)}) / q(h^{(n)} | x) \nonumber \\
\tilde{w}^{(n)} &= w^{(n)} / \sum_n w^{(n)} \nonumber \\
\mu_{t+1} &= \sum_n \tilde{w}^{(n)} h^{(n)}
\end{align}

This will clearly lead to a biased estimate of the lower bound. But we will show that it works very well in practice.

\section{Related Works}
\alert{This needs work}
Our work combines elements found in variational inference and MCMC methods. In spirit, it is closest to the hybrid MCMC for variational inference \citep{salimans2014markov}, but uses a much MCMC transition than the hamiltonian MC used there, while the AdIS method is also applicable to binary units.

Recent work on stochastic variational inference \cite{hoffman2013stochastic} also uses gradient descent to improve on the variational inference algorithm. However, the exact parameterization of the posterior is very limited and the method cannot be used with complex approximate posteriors. 

Normalizing flows for VAE \citep{rezende2015variational} make use of auxiliary networks to improve the approximate posterior. In doing so, they are able to escape the factorial condition that GDIR and AdISIR have. But they require additional parameters to improve the posterior.

\section{Experiments}

\subsection{Settings}

We test GDIR and AdISIR on MNIST, using the binarized dataset found in \citep{salakhutdinov2008quantitative}, with a standard train, validation, and test split of $50$k, $10$k, and $10$k samples. Unfortunately, recent papers on VAE \citep{mnih2014neural, salimans2014markov} have used different, non-standard splits, or actively sample from continuous MNIST \citep{burda2015importance}, making it difficult to compete. We however make sure that our results are from the most conservative setting (having the least amount of training examples), thereby making any comparison as meaningful as possible.

All models used a recognition network, for comparison to similar models, with the number of parameters and deterministic layers (if present) to match the generation network.

All models were trained with early stopping by recording best validation, and none of our models used any of the regularizations available to us, such as dropout, weight noise, or L2-norm regularization. All of our models were trained for at most $500$ epochs, and each took no more than $24$ hours to complete, best validation was typically reached much faster.

\subsection{Continuous Variables}
For the continuous latent variable case we used the same network structure as in \citep{kingma2013auto, salimans2014markov}, with $200$ continuous latent variables and a generation and recognition network with $500$ deterministic softplus units. Each of our models were trained using $20$ samples during inference using GDIR to refine output of the recognition network and $20$ samples to approximate the lower bound during the M-step. Optimal parameters were found using the rmsprop algorithm \citep{Hinton-Coursera2012} with an initial learning rate of \num{0.0001} with a batch size of $100$. \alert{We are currently running validation exps to pick new hps}

Our results with continuous variables are presented in table \ref{table:continuous}. For comparison are (approximate) VAE results \alert{They just have plots, no numbers} from \citep{kingma2013auto}, normalizing flow \citep{rezende2015variational}, Hamiltonian MCMC variational inference \citep{salimans2014markov} with the number of leapfrog steps specified.

We observe faster convergence and better lower bound when using more gradient steps. \alert{I need to add these tables}

GDIR shows notable improvement over VAE, while showing comparable performance to Hamiltonian MCMC, despite conservative training. \alert{Need to read more on normalizing flow, but it's unclear the parameterization they use, number of latent layers, and whether they are reporting NLL or lower bound. In addition they have more parameters in their auxiliary inference network, so this needs to be addressed.}

\begin{table}
\label{table:continuous}
\begin{tabular}{ | m{5em} | m{1cm}| m{5cm} | } 
\hline
Model & lower bound & NLL \\ 
\hline
\hline
VAE & 99 &  \\ 
\hline
$GBVI_{20}$ & 92.83 & 90.51 \\ 
\hline
\hline
$HVI_5$\footnote[1]{Using the Larochelle dataset and validation dataset during training} &  90.86 & 87.16 \\ 
$HVI_{10}$\footnote[1] & 87.60 & 85.56 \\ 
$DLGM+NF$ & \alert{TODO} & \alert{results in paper are confusing} \\
\hline
\end{tabular}
\caption{Lower bounds and NLL for various continuous latent variable models and training algorithms.}
\end{table}

\subsection{Bernoulli Variables}
For binary latent variables, we used two structures to compare to wake-sleep and NVIL. For comparison to wake-sleep, we used a shallow feed forward network with $200$ output neurons for the binary latent variables for the posterior and no deterministic units for the posterior nor conditional. As NVIL uses an additional feed forward network for baseline prediction, we used an deterministic layer of $240$ softplus units in the recognition and generation networks to roughly match the total number of parameters. Parameters were adjusted using Adam \citep{kingma2014method} with an initial learning rate of \num{0.0003} with a batch size of $10$ as per validation. \alert{Much of these hps will change this week, most likely to match continuous above}

\begin{table}
\label{table:binary}
\begin{tabular}{ | m{6em} | m{1cm}| m{1cm} | } 
\hline
Model & lower bound & NLL \\ 
\hline
\hline
Wake-sleep & 120.7 & 116.3 \\ 
\hline
NVIL & 113.1 &  \\ 
\hline
$AVI_{20}$ & 114.08 & 110.13 \\
$AVI_{20}(240)$ & 107.03 & 103.10 \\
\hline
$MPVI_{20}$ &  &  \\
$MPVI_{20}(240)$ & 122.67  & 103.32 \\
\hline
\end{tabular}
\caption{Test lower bounds and NLL for various Bernoulli latent variable models training algorithms.}
\end{table}

\alert{We are clearly beating NVIL and wake-sleep. Though with no extra deterministic layers we are also very close. The story with number of steps is not as strong as above. We can reach the same lower bound with only one step. But it's MUCH faster to use more steps. I will work on these figures. In addition, I am working on code / exps with 2 layers of latent variables: I have a feeling the difference there will be more pronounced.}

\section{Conclusion}

\alert{TBD}


\subsubsection*{Acknowledgments}
Microsoft, Mind Research Grants, PIBBS maybe

\alert{TODO}


\bibliography{ais_vae}
\bibliographystyle{iclr2016_conference}

\end{document}
